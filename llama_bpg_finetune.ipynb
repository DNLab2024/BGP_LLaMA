{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "# %pip install -q datasets bitsandbytes einops wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from typing import List\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "from datasets import load_dataset\n",
    "import os\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# import seaborn as sns\n",
    "from pylab import rcParams\n",
    "# from trl import SFTTrainer\n",
    " \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1de3523a5f4626ba80bdfc8cae0ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Need auth token for these\n",
    "hf_auth = os.environ.get('hf_token')\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    " \n",
    "# tokenizer.pad_token_id = (0)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/hb/.cache/huggingface/datasets/json/default-954e161ec1be7a92/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9c9ec110bf490795e294a1cba0b775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 14866\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"/home/hb/LLM-research/finetuning_dataset/BGP_knowledge.json\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN = 512\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    " \n",
    " \n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    " \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    " \n",
    "    return result\n",
    " \n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/hb/.cache/huggingface/datasets/json/default-954e161ec1be7a92/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-bda3c556bbad67b1.arrow and /home/hb/.cache/huggingface/datasets/json/default-954e161ec1be7a92/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-64ebe0635b6e7740.arrow\n",
      "Loading cached processed dataset at /home/hb/.cache/huggingface/datasets/json/default-954e161ec1be7a92/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6c598db7cfe4c632.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba40ca8484134d8885a3b0a556bc5db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=1400, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    train_val[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    " \n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "output_dir = \"./llama_BGP_10\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 500\n",
    "logging_steps = 200\n",
    "learning_rate = 1e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 20000\n",
    "warmup_ratio = 0.05\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    # save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, return_tensors=\"pt\", padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/myenv/lib/python3.8/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/home/hb/myenv/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:166: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Loading cached processed dataset at /home/hb/.cache/huggingface/datasets/json/default-954e161ec1be7a92/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a822c1153aac6c48.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b863efa89754bd8843a1b684e0c662f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = None\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"output\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hb/myenv/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/hb/myenv/lib/python3.8/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Explain what BGP blackholing activity is [/INST]  BGP blackholing activity is a malicious practice in which an attacker injects false BGP routes into the network in order to redirect traffic away from its intended destination. This can be used to disrupt services, intercept traffic, or even launch denial-of-service attacks. BGP blackholing activity can be difficult to detect, as it often involves subtle changes to existing routes or the introduction of new routes. To protect against BGP blackholing activity, network operators should implement security measures such as route filtering,\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Explain what BGP blackholing activity is\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama2-13b-BGP_10/tokenizer_config.json',\n",
       " 'llama2-13b-BGP_10/special_tokens_map.json',\n",
       " 'llama2-13b-BGP_10/tokenizer.model',\n",
       " 'llama2-13b-BGP_10/added_tokens.json',\n",
       " 'llama2-13b-BGP_10/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "new_model = \"llama2-13b-BGP_10\"\n",
    "\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True-False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "input_file_path = \"/home/hb/fine-tuning-alpaca/evaluation/bgp_test_true_false.json\" \n",
    "\n",
    "# Load questions and answers from JSON file\n",
    "with open(input_file_path, \"r\") as input_file:\n",
    "    questions_data = json.load(input_file)\n",
    "\n",
    "# Initialize text generation pipeline\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "\n",
    "llama2_answers = []\n",
    "\n",
    "# Evaluate generated answers\n",
    "correct_answers = 0\n",
    "total_questions = len(questions_data)\n",
    "\n",
    "for q_dict in questions_data:\n",
    "    question = q_dict[\"question\"]\n",
    "    answer = q_dict[\"answer\"]\n",
    "\n",
    "    # Generate answer using the language model\n",
    "    prompt = f\"[INST] {question} [/INST]\"\n",
    "    generated_answer = pipe(prompt)[0]['generated_text']\n",
    "    # print(generated_answer)\n",
    "    llama2_answers.append(generated_answer)\n",
    "\n",
    "# Extract ground truth answers\n",
    "ground_truth = [q_dict[\"answer\"] for q_dict in questions_data]\n",
    "\n",
    "print(ground_truth)\n",
    "\n",
    "for ground_answer, generated_answer in zip(ground_truth, llama2_answers):\n",
    "    generated_answer = re.search(r'\\[/INST\\]\\s+(.*?)\\.', generated_answer).group(1).strip()\n",
    "    print(f\"Generated Answer: {generated_answer}\")\n",
    "    \n",
    "    if generated_answer.lower() == ground_answer.lower():\n",
    "        print(\"Correct\")\n",
    "        correct_answers += 1\n",
    "    else:\n",
    "        print(\"Incorrect\")\n",
    "\n",
    "accuracy = (correct_answers / total_questions) * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Correct answers: {correct_answers}\")\n",
    "print(f\"Incorrect answers: {total_questions - correct_answers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_file_path = \"/home/hb/fine-tuning-alpaca/evaluation/bgp_test_fill_the_blank.json\" \n",
    "\n",
    "with open(input_file_path, \"r\") as input_file:\n",
    "    questions_data = json.load(input_file)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "\n",
    "generated_answers_fb = []\n",
    "filled_answers = []\n",
    "\n",
    "n_question = 0\n",
    "for q_dict in questions_data:\n",
    "    question = q_dict[\"question\"]\n",
    "    answer = q_dict[\"answer\"]\n",
    "\n",
    "    # Generate answer using the language model\n",
    "    prompt = f\"[INST]{question} \\n Fill the blank: \\n [/INST]\"\n",
    "    generated_answer = pipe(prompt)[0]['generated_text']\n",
    "    # print(f\"Output: {generated_answer}\")\n",
    "    \n",
    "    generated_answer = re.search(r'\\[/INST\\]\\s+(.*)', generated_answer).group(1).strip()\n",
    "    if \"Sure\" in generated_answer:\n",
    "        generated_lines = generated_answer.split('\\n')\n",
    "        if len(generated_lines) > 1:\n",
    "            generated_answer = '\\n'.join(generated_lines[1:])  # Skip the first line\n",
    "    generated_answers_fb.append(generated_answer)\n",
    "    print(f\"Generated answer: {generated_answer}\")\n",
    "    # Find the position of the placeholder in the first string\n",
    "    placeholder_position = question.find(\"________\")\n",
    "\n",
    "    # Extract the content filled in the placeholder\n",
    "    filled_content = generated_answer[placeholder_position:placeholder_position + len(\"autonomous systems (ASes)\")]\n",
    "    filled_answers.append(filled_content)\n",
    "    n_question += 1\n",
    "    print(f\"Filled content: {filled_content}\")\n",
    "    print(f\"---------------------{n_question}-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_file_path = \"/home/hb/fine-tuning-alpaca/evaluation/bgp_test_multiple_choice.json\" \n",
    "\n",
    "with open(input_file_path, \"r\") as input_file:\n",
    "    questions_data = json.load(input_file)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=128)\n",
    "\n",
    "generated_answers = []\n",
    "ground_truth = [q_dict[\"answer\"] for q_dict in questions_data]\n",
    "n_question = 0\n",
    "\n",
    "for q_dict in questions_data:\n",
    "    question = q_dict[\"question\"]\n",
    "    options = q_dict[\"options\"]\n",
    "    \n",
    "    # Construct prompt with question and options\n",
    "    prompt = f\"Choose the correct answer: \\n{question}\\n\" + \"\\n\".join(options)\n",
    "    # print(prompt)\n",
    "    \n",
    "    # Generate answer using the language model\n",
    "    generated_answer = pipe(prompt)[0]['generated_text']\n",
    "    print(f\"Generated Answer: {generated_answer}\")\n",
    "    \n",
    "    # Parse the generated answer\n",
    "    answer_lines = generated_answer.splitlines()\n",
    "    parsed_answer = \"\"\n",
    "    for line in answer_lines:\n",
    "        if line.startswith(\"Answer: \") or line.startswith(\"Correct answer: \"):\n",
    "            parsed_answer = re.search(r'\\b(\\w)\\)', line).group(1)\n",
    "            print(f\"Parsed_answer: {parsed_answer}\")\n",
    "            break\n",
    "    generated_answers.append(parsed_answer)\n",
    "    n_question += 1\n",
    "    print(f\"----------------{n_question}-----------------\")\n",
    "print(generated_answers)\n",
    "\n",
    "\n",
    "total_questions = len(questions_data)\n",
    "correct_answers = 0\n",
    "\n",
    "for ground_answer, generated_answer in zip(ground_truth, generated_answers):\n",
    "    if generated_answer.lower() == ground_answer.lower():\n",
    "        print(\"Correct\")\n",
    "        correct_answers += 1\n",
    "    else:\n",
    "        print(\"Incorrect\")\n",
    "        print(f\"LLaMA answer: {generated_answer}. Correct answer: {ground_answer}\")    \n",
    "accuracy = (correct_answers / total_questions) * 100\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Correct answers: {correct_answers}\")\n",
    "print(f\"Incorrect answers: {total_questions - correct_answers}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2b80e498404ab5b55d412eb6aa3201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "new_model = \"llama2-13b-BGP_10\"\n",
    "\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1dc9ee24c84e25bbc874e16ad4f532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49367edbf414f0285dcee8c406c7cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c618ecd8bd6a4c5c9ef379b391aaaad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a26f1bbb79845639cfed36fe23d9bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e4006023f548c4874ebd70dbc2b520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccae8954d324563a66edd00ed440bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hyonbokan/BGP-llama_20k-constant/commit/4189b242574f1db0a6a190e157752d00b2875d0e', commit_message='Upload tokenizer', commit_description='', oid='4189b242574f1db0a6a190e157752d00b2875d0e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('hyonbokan/BGP-llama_20k-constant')\n",
    "tokenizer.push_to_hub('hyonbokan/BGP-llama_20k-constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
